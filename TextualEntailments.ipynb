{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16789,"status":"ok","timestamp":1643799755396,"user":{"displayName":"Alessandro Ballerini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhO_HdtECBqJW2VCk6nU86LuH0kwVRvP3Mj-OU2og=s64","userId":"02001153230010659820"},"user_tz":-60},"id":"gGD8iO9SiUCi","outputId":"cf83ae0b-a6fc-4fef-dc1e-92618b47a2e5"},"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["# import re\n","import random\n","import pickle\n","\n","from io import open\n","from tqdm import tqdm\n","\n","# Torch\n","import torch\n","import torch.nn as nn\n","from torch import optim\n","import torch.nn.functional as F\n","\n","# Word Tokenizere\n","import nltk\n","nltk.download('punkt')\n","from nltk.tokenize import word_tokenize\n","\n","# Mount google drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Sentence similarity evaluation\n","import spacy\n","#!python -m spacy download en_core_web_lg\n","nlp = spacy.load(\"en_core_web_lg\")\n","\n","# Set CUDA as device if available\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19,"status":"ok","timestamp":1643799755397,"user":{"displayName":"Alessandro Ballerini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhO_HdtECBqJW2VCk6nU86LuH0kwVRvP3Mj-OU2og=s64","userId":"02001153230010659820"},"user_tz":-60},"id":"YLDZIx3Ci5UO","outputId":"eeca52a5-e4a3-4992-e28f-0489aa452c1f"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda')"]},"metadata":{},"execution_count":2}],"source":["device"]},{"cell_type":"markdown","metadata":{"id":"0DSOERj1y5mt"},"source":["#### Load the dataset"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"ePMkJNUSi_Xy","executionInfo":{"status":"ok","timestamp":1643799755798,"user_tz":-60,"elapsed":413,"user":{"displayName":"Alessandro Ballerini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhO_HdtECBqJW2VCk6nU86LuH0kwVRvP3Mj-OU2og=s64","userId":"02001153230010659820"}}},"outputs":[],"source":["with open('/content/drive/MyDrive/InforetProject/Datasets/entails_dataset.pickle', 'rb') as handle:\n","    entails_dataset = pickle.load(handle)"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"33ySP8XvzrTq","executionInfo":{"status":"ok","timestamp":1643799756044,"user_tz":-60,"elapsed":250,"user":{"displayName":"Alessandro Ballerini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhO_HdtECBqJW2VCk6nU86LuH0kwVRvP3Mj-OU2og=s64","userId":"02001153230010659820"}}},"outputs":[],"source":["with open('/content/drive/MyDrive/InforetProject/Datasets/imdb_dataset.pickle', 'rb') as handle:\n","    imdb_dataset = pickle.load(handle)"]},{"cell_type":"markdown","metadata":{"id":"8QUDLfRJy8ta"},"source":["#### Create a corpus using the dataset"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"YmLr8kFpjJvu","executionInfo":{"status":"ok","timestamp":1643799756045,"user_tz":-60,"elapsed":8,"user":{"displayName":"Alessandro Ballerini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhO_HdtECBqJW2VCk6nU86LuH0kwVRvP3Mj-OU2og=s64","userId":"02001153230010659820"}}},"outputs":[],"source":["SOS_token = 0\n","EOS_token = 1\n","\n","class Corpus:\n","    def __init__(self):\n","        self.word2index = {}\n","        self.word2count = {}\n","        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n","        self.n_words = 2  # Count SOS and EOS\n","\n","    def addSentence(self, sentence):\n","        for word in word_tokenize(sentence):\n","            self.addWord(word)\n","\n","    def addWord(self, word):\n","        if word not in self.word2index:\n","            self.word2index[word] = self.n_words\n","            self.word2count[word] = 1\n","            self.index2word[self.n_words] = word\n","            self.n_words += 1\n","        else:\n","            self.word2count[word] += 1\n","\n","def indexesFromSentence(corpus, sentence):\n","    return [corpus.word2index[word] for word in word_tokenize(sentence)]\n","\n","def tensorFromSentence(corpus, sentence):\n","    indexes = indexesFromSentence(corpus, sentence)\n","    indexes.append(EOS_token)\n","    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n","\n","def tensorsFromPair(pair):\n","    input_tensor = tensorFromSentence(corpus, pair[0])\n","    target_tensor = tensorFromSentence(corpus, pair[1])\n","    return (input_tensor, target_tensor)"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"ZLrLNNSUjixn","executionInfo":{"status":"ok","timestamp":1643799758788,"user_tz":-60,"elapsed":2749,"user":{"displayName":"Alessandro Ballerini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhO_HdtECBqJW2VCk6nU86LuH0kwVRvP3Mj-OU2og=s64","userId":"02001153230010659820"}}},"outputs":[],"source":["corpus = Corpus()\n","MAX_LENGTH = 20\n","dataset = []\n","\n","for pair in entails_dataset:\n","    corpus.addSentence(pair[0])\n","    corpus.addSentence(pair[1])\n","    if len(word_tokenize(pair[0])) < MAX_LENGTH and len(word_tokenize(pair[1])) < MAX_LENGTH:\n","        dataset.append(pair)\n","\n","# Increase the number of words in the corpus\n","for text in imdb_dataset[100]:\n","    corpus.addSentence(text)"]},{"cell_type":"markdown","metadata":{"id":"Fhukq3SWFQ8e"},"source":["#### Divide training set and test set"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"_-ktesPityeT","executionInfo":{"status":"ok","timestamp":1643799758789,"user_tz":-60,"elapsed":21,"user":{"displayName":"Alessandro Ballerini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhO_HdtECBqJW2VCk6nU86LuH0kwVRvP3Mj-OU2og=s64","userId":"02001153230010659820"}}},"outputs":[],"source":["random.shuffle(dataset)\n","trainingSetSize = len(dataset) // 100 * 80\n","\n","trainingSet = dataset[:trainingSetSize]\n","testSet = dataset[trainingSetSize:]"]},{"cell_type":"markdown","metadata":{"id":"BRjWbzbXzYFA"},"source":["#### Define Encoder and Decoder"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"9M22I5TAk7Wi","executionInfo":{"status":"ok","timestamp":1643799758791,"user_tz":-60,"elapsed":20,"user":{"displayName":"Alessandro Ballerini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhO_HdtECBqJW2VCk6nU86LuH0kwVRvP3Mj-OU2og=s64","userId":"02001153230010659820"}}},"outputs":[],"source":["class EncoderRNN(nn.Module):\n","    def __init__(self, input_size, hidden_size):\n","        super(EncoderRNN, self).__init__()\n","        self.hidden_size = hidden_size\n","\n","        self.embedding = nn.Embedding(input_size, hidden_size)\n","        self.gru = nn.GRU(hidden_size, hidden_size)\n","\n","    def forward(self, input, hidden):\n","        embedded = self.embedding(input).view(1, 1, -1)\n","        output = embedded\n","        output, hidden = self.gru(output, hidden)\n","        return output, hidden\n","\n","    def initHidden(self):\n","        return torch.zeros(1, 1, self.hidden_size, device=device)"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"kAF1YdRttVy_","executionInfo":{"status":"ok","timestamp":1643799758792,"user_tz":-60,"elapsed":20,"user":{"displayName":"Alessandro Ballerini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhO_HdtECBqJW2VCk6nU86LuH0kwVRvP3Mj-OU2og=s64","userId":"02001153230010659820"}}},"outputs":[],"source":["class AttnDecoderRNN(nn.Module):\n","    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n","        super(AttnDecoderRNN, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.output_size = output_size\n","        self.dropout_p = dropout_p\n","        self.max_length = max_length\n","\n","        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n","        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n","        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n","        self.dropout = nn.Dropout(self.dropout_p)\n","        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n","        self.out = nn.Linear(self.hidden_size, self.output_size)\n","\n","    def forward(self, input, hidden, encoder_outputs):\n","        embedded = self.embedding(input).view(1, 1, -1)\n","        embedded = self.dropout(embedded)\n","\n","        attn_weights = F.softmax(self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n","        attn_applied = torch.bmm(attn_weights.unsqueeze(0),encoder_outputs.unsqueeze(0))\n","\n","        output = torch.cat((embedded[0], attn_applied[0]), 1)\n","        output = self.attn_combine(output).unsqueeze(0)\n","\n","        output = F.relu(output)\n","        output, hidden = self.gru(output, hidden)\n","\n","        output = F.log_softmax(self.out(output[0]), dim=1)\n","        return output, hidden, attn_weights\n","\n","    def initHidden(self):\n","        return torch.zeros(1, 1, self.hidden_size, device=device)"]},{"cell_type":"markdown","metadata":{"id":"JGi0KJ96zcfk"},"source":["#### Training Algorithm"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"X4Jv2cX_uOVq","executionInfo":{"status":"ok","timestamp":1643799758793,"user_tz":-60,"elapsed":19,"user":{"displayName":"Alessandro Ballerini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhO_HdtECBqJW2VCk6nU86LuH0kwVRvP3Mj-OU2og=s64","userId":"02001153230010659820"}}},"outputs":[],"source":["teacher_forcing_ratio = 0.5\n","\n","def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, train=True):\n","    encoder_hidden = encoder.initHidden()\n","\n","    encoder_optimizer.zero_grad()\n","    decoder_optimizer.zero_grad()\n","\n","    input_length = input_tensor.size(0)\n","    target_length = target_tensor.size(0)\n","\n","    encoder_outputs = torch.zeros(MAX_LENGTH, \n","                                  encoder.hidden_size, \n","                                  device=device)\n","\n","    loss = 0\n","\n","    for ei in range(input_length):\n","        encoder_output, encoder_hidden = encoder(\n","            input_tensor[ei], \n","            encoder_hidden)\n","        encoder_outputs[ei] = encoder_output[0, 0]\n","\n","    decoder_input = torch.tensor([[SOS_token]], device=device)\n","    decoder_hidden = encoder_hidden\n","    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n","\n","    # use_teacher_forcing = False\n","\n","    if use_teacher_forcing:\n","        # Teacher forcing: Feed the target as the next input\n","        for di in range(target_length):\n","            decoder_output, decoder_hidden, decoder_attention = decoder(\n","                decoder_input, \n","                decoder_hidden, \n","                encoder_outputs)\n","            loss += criterion(decoder_output, target_tensor[di])\n","            decoder_input = target_tensor[di]  # Teacher forcing\n","\n","    else:\n","        # Without teacher forcing: use its own predictions as the next input\n","        for di in range(target_length):\n","            decoder_output, decoder_hidden, decoder_attention = decoder(\n","                decoder_input, \n","                decoder_hidden, \n","                encoder_outputs)\n","            topv, topi = decoder_output.topk(1)\n","            decoder_input = topi.squeeze().detach()  # detach from history as input\n","\n","            loss += criterion(decoder_output, target_tensor[di])\n","            if decoder_input.item() == EOS_token:\n","                break\n","\n","    if train:\n","        loss.backward()\n","        encoder_optimizer.step()\n","        decoder_optimizer.step()\n","\n","    return loss.item() / target_length"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"km9KooKlvFpZ","executionInfo":{"status":"ok","timestamp":1643799758795,"user_tz":-60,"elapsed":19,"user":{"displayName":"Alessandro Ballerini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhO_HdtECBqJW2VCk6nU86LuH0kwVRvP3Mj-OU2og=s64","userId":"02001153230010659820"}}},"outputs":[],"source":["def trainIters(trainingSet, encoder, decoder, testSet=None, n_iters=1000, plot_every=100, learning_rate=0.01, test_ratio=0.2, test=False):\n","    plot_losses = []\n","    plot_loss_total = 0\n","\n","    plot_test_losses = []\n","    plot_test_loss_total = 0\n","\n","    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n","    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n","    training_pairs = [tensorsFromPair(random.choice(trainingSet)) for i in range(n_iters)]\n","    criterion = nn.NLLLoss()\n","\n","    for iter in tqdm(range(1, n_iters + 1)):\n","        training_pair = training_pairs[iter - 1]\n","        input_tensor = training_pair[0]\n","        target_tensor = training_pair[1]\n","\n","        loss = train(input_tensor, \n","                     target_tensor, \n","                     encoder,\n","                     decoder, \n","                     encoder_optimizer, \n","                     decoder_optimizer, \n","                     criterion)\n","        plot_loss_total += loss\n","        \n","        if iter % plot_every == 0:\n","            plot_loss_avg = plot_loss_total / plot_every\n","            plot_losses.append(plot_loss_avg)\n","            plot_loss_total = 0\n","\n","            if test:\n","                test_set_size = int(n_iters // 100 * (test_ratio*100))\n","                testing_pairs = [tensorsFromPair(random.choice(testSet)) for i in range(test_set_size)]\n","\n","                for test_iter in range(1, test_set_size + 1):\n","                    testing_pair = testing_pairs[test_set_size - 1]\n","                    input_tensor = testing_pair[0]\n","                    target_tensor = testing_pair[1]\n","\n","                    loss = train(input_tensor, \n","                                target_tensor, \n","                                encoder,\n","                                decoder, \n","                                encoder_optimizer, \n","                                decoder_optimizer, \n","                                criterion,\n","                                train=False)\n","                    plot_test_loss_total += loss\n","\n","                plot_loss_avg = plot_test_loss_total / test_set_size\n","                plot_test_losses.append(plot_loss_avg)\n","                plot_test_loss_total = 0\n","            \n","    if test:\n","        return plot_losses, plot_test_losses\n","    else:\n","        return plot_losses"]},{"cell_type":"markdown","metadata":{"id":"3WyUdiMZOj-o"},"source":["#### Evaluation"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"ErAHKg6ivK1H","executionInfo":{"status":"ok","timestamp":1643799758796,"user_tz":-60,"elapsed":18,"user":{"displayName":"Alessandro Ballerini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhO_HdtECBqJW2VCk6nU86LuH0kwVRvP3Mj-OU2og=s64","userId":"02001153230010659820"}}},"outputs":[],"source":["def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n","    with torch.no_grad():\n","        input_tensor = tensorFromSentence(corpus, sentence)\n","        input_length = input_tensor.size()[0]\n","        encoder_hidden = encoder.initHidden()\n","\n","        encoder_outputs = torch.zeros(\n","            max_length, \n","            encoder.hidden_size, \n","            device=device)\n","        \n","        for ei in range(input_length):\n","            encoder_output, encoder_hidden = encoder(\n","                input_tensor[ei],\n","                encoder_hidden)\n","            encoder_outputs[ei] += encoder_output[0, 0]\n","\n","        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n","        decoder_hidden = encoder_hidden\n","        decoded_words = []\n","        decoder_attentions = torch.zeros(max_length, max_length)\n","\n","        for di in range(max_length):\n","            decoder_output, decoder_hidden, decoder_attention = decoder(\n","                decoder_input, \n","                decoder_hidden, \n","                encoder_outputs)\n","            \n","            decoder_attentions[di] = decoder_attention.data\n","            topv, topi = decoder_output.data.topk(1)\n","            if topi.item() == EOS_token:\n","                decoded_words.append('<EOS>')\n","                break\n","            else:\n","                decoded_words.append(corpus.index2word[topi.item()])\n","\n","            decoder_input = topi.squeeze().detach()\n","\n","        return decoded_words, decoder_attentions[:di + 1]\n","\n","def evaluateRandomly(encoder, decoder, pairs, n=100, print_output=True, output=False):\n","    out = []\n","    for i in range(n):\n","        pair = random.choice(pairs)\n","        output_words, attentions = evaluate(encoder, decoder, pair[0])\n","        output_sentence = ' '.join(output_words)\n","        if print_output:\n","            print('>', pair[0], '\\n=', pair[1], '\\n<', output_sentence, '\\n')\n","        if output:\n","            out.append((pair[0], pair[1], output_sentence))\n","    if output: return out\n","\n","def evaluateSentence(sentence):\n","    output_words, attentions = evaluate(encoder, attn_decoder, sentence)\n","    output_sentence = ' '.join(output_words)\n","    print('>', sentence, '\\n<', output_sentence, '\\n')\n","\n","def evaluate_model(pairs, n_samples=100):\n","    sim = []\n","    eva = evaluateRandomly(encoder, attn_decoder, pairs, n=n_samples, print_output=False, output=True)\n","    for e in eva:\n","        _, target, answer = e\n","        sim.append(nlp(target).similarity(nlp(answer)))           \n","    return sum(sim) / len(sim)"]},{"cell_type":"markdown","metadata":{"id":"Yn-xoPwLOoU6"},"source":["#### Model Training"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"N3Zv8HVsvQX6","executionInfo":{"status":"ok","timestamp":1643799768283,"user_tz":-60,"elapsed":9504,"user":{"displayName":"Alessandro Ballerini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhO_HdtECBqJW2VCk6nU86LuH0kwVRvP3Mj-OU2og=s64","userId":"02001153230010659820"}}},"outputs":[],"source":["hidden_size = 256\n","\n","encoder = EncoderRNN(corpus.n_words, hidden_size).to(device)\n","attn_decoder = AttnDecoderRNN(hidden_size, \n","                              corpus.n_words, \n","                              dropout_p=0.1).to(device)\n","\n","train_loss = trainIters(trainingSet, encoder, attn_decoder, n_iters=75000, plot_every=7500, test=False)"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"J5BS1BETC-lI","executionInfo":{"status":"ok","timestamp":1643799768298,"user_tz":-60,"elapsed":34,"user":{"displayName":"Alessandro Ballerini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhO_HdtECBqJW2VCk6nU86LuH0kwVRvP3Mj-OU2og=s64","userId":"02001153230010659820"}},"outputId":"0f2136d0-0bf0-455a-d15e-252f63ee7f6d"},"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'import matplotlib.pyplot as plt\\nfig, ax = plt.subplots(figsize=(12,3))\\nax.plot(train_loss, \"r\", marker=\\'o\\')\\n# ax.plot(test_loss, \"g\", marker=\\'o\\')\\nplt.show()'"]},"metadata":{},"execution_count":14}],"source":["import matplotlib.pyplot as plt\n","fig, ax = plt.subplots(figsize=(12,3))\n","ax.plot(train_loss, \"r\", marker='o')\n","plt.show()"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"weO05ombimk2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1643799768954,"user_tz":-60,"elapsed":681,"user":{"displayName":"Alessandro Ballerini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhO_HdtECBqJW2VCk6nU86LuH0kwVRvP3Mj-OU2og=s64","userId":"02001153230010659820"}},"outputId":"21cfdd51-ab2c-4c8b-83a2-5af94e4127ae"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":16}],"source":["encoderStatePath = \"/content/drive/MyDrive/InforetProject/ModelState/textualEntails_encoder.pth\"\n","decoderStatePath = \"/content/drive/MyDrive/InforetProject/ModelState/textualEntails_decoder.pth\"\n","# torch.save(encoder.state_dict(), encoderStatePath)\n","# torch.save(attn_decoder.state_dict(), decoderStatePath)\n","encoder.load_state_dict(torch.load(encoderStatePath))\n","attn_decoder.load_state_dict(torch.load(decoderStatePath))"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"xSdg2Ykfnc9Y","executionInfo":{"status":"ok","timestamp":1643799769654,"user_tz":-60,"elapsed":704,"user":{"displayName":"Alessandro Ballerini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhO_HdtECBqJW2VCk6nU86LuH0kwVRvP3Mj-OU2og=s64","userId":"02001153230010659820"}}},"outputs":[],"source":["train_out = evaluateRandomly(encoder, attn_decoder, trainingSet, 100, output=True, print_output=False)\n","trainOutput = \"/content/drive/MyDrive/InforetProject/trainOutput.txt\"\n","f = open(trainOutput, \"a\")\n","for tri in train_out:\n","    f.write(tri[0]+'\\n')\n","    f.write(tri[1]+'\\n')\n","    f.write(tri[2]+'\\n\\n')\n","f.close()"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"in0z1YQXoPQr","executionInfo":{"status":"ok","timestamp":1643799770322,"user_tz":-60,"elapsed":675,"user":{"displayName":"Alessandro Ballerini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhO_HdtECBqJW2VCk6nU86LuH0kwVRvP3Mj-OU2og=s64","userId":"02001153230010659820"}}},"outputs":[],"source":["test_out = evaluateRandomly(encoder, attn_decoder, testSet, 100, output=True, print_output=False)\n","testOutput = \"/content/drive/MyDrive/InforetProject/testOutput.txt\"\n","f = open(testOutput, \"a\")\n","for tri in test_out:\n","    f.write(tri[0]+'\\n')\n","    f.write(tri[1]+'\\n')\n","    f.write(tri[2]+'\\n\\n')\n","f.close()"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3iZmhNGY5gjV","executionInfo":{"status":"ok","timestamp":1643799770323,"user_tz":-60,"elapsed":27,"user":{"displayName":"Alessandro Ballerini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhO_HdtECBqJW2VCk6nU86LuH0kwVRvP3Mj-OU2og=s64","userId":"02001153230010659820"}},"outputId":"023968a6-8738-461d-ed7c-8daa5d7c8f9d"},"outputs":[{"output_type":"stream","name":"stdout","text":["> Food is good enough \n","< food is be <EOS> \n","\n"]}],"source":["evaluateSentence(\"Food is good enough\")"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Bqqm1cmdpNEN","executionInfo":{"status":"ok","timestamp":1643799770325,"user_tz":-60,"elapsed":17,"user":{"displayName":"Alessandro Ballerini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhO_HdtECBqJW2VCk6nU86LuH0kwVRvP3Mj-OU2og=s64","userId":"02001153230010659820"}},"outputId":"af5b5d44-9651-41b2-ad22-1b308cf63bce"},"outputs":[{"output_type":"stream","name":"stdout","text":["> There is enough leg room \n","< The seats could do with more legroom <EOS> \n","\n"]}],"source":["evaluateSentence(\"There is enough leg room\")"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"tK_4fhN8pXkC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1643799814563,"user_tz":-60,"elapsed":346,"user":{"displayName":"Alessandro Ballerini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhO_HdtECBqJW2VCk6nU86LuH0kwVRvP3Mj-OU2og=s64","userId":"02001153230010659820"}},"outputId":"f066355f-d642-4cc6-cc9a-e1598662f291"},"outputs":[{"output_type":"stream","name":"stdout","text":["> I would like a fruit \n","< Food food be with more choice <EOS> \n","\n"]}],"source":["evaluateSentence(\"I would like a fruit\")"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"t0zX9udMuJxN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1643799829253,"user_tz":-60,"elapsed":268,"user":{"displayName":"Alessandro Ballerini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhO_HdtECBqJW2VCk6nU86LuH0kwVRvP3Mj-OU2og=s64","userId":"02001153230010659820"}},"outputId":"745ddba8-35ce-42f7-ad02-ffd9f413c5ad"},"outputs":[{"output_type":"stream","name":"stdout","text":["> There is not enough time \n","< food is average <EOS> \n","\n"]}],"source":["evaluateSentence(\"There is not enough time\")"]},{"cell_type":"markdown","metadata":{"id":"ISnrMP3gEPs3"},"source":["### Model Evaluatioin\n","\n","Performance on training set\n","\n","Performance on test set\n","\n","Ability to generalize "]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["BRjWbzbXzYFA"],"name":"TextualEntails.ipynb","provenance":[],"authorship_tag":"ABX9TyOAZku/JNmAVOSIG7hLfrE7"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}